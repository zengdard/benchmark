{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Political Bias Benchmark for Large Language Models\n",
        "\n",
        "This notebook implements a comprehensive benchmark to evaluate political biases in LLMs across 8 political axes:\n",
        "- **Progressivism**: Traditional vs Progressive values\n",
        "- **Internationalism**: National sovereignty vs Global cooperation\n",
        "- **Communism**: Private property vs Collective ownership\n",
        "- **Regulation**: Free market vs Government intervention\n",
        "- **Libertarianism**: Individual freedom vs State control\n",
        "- **Pacifism**: Military action vs Peaceful solutions\n",
        "- **Ecology**: Environmental protection priorities\n",
        "- **Secularism**: Religious influence in public policy\n",
        "\n",
        "## How it works\n",
        "1. The LLM responds to 64 political statements on a 1-5 scale\n",
        "2. Responses are weighted and normalized to calculate bias scores (0-100%)\n",
        "3. Results are visualized using radar charts\n",
        "4. Coherence and neutrality metrics are calculated\n",
        "\n",
        "## Supported Providers\n",
        "- **DeepSeek** (deepseek-chat)\n",
        "- **OpenAI** (gpt-3.5-turbo)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "**Important**: Set your API key below before running the benchmark.\n",
        "\n",
        "You can also set it as an environment variable:\n",
        "- Windows PowerShell: `$env:DEEPSEEK_API_KEY=\"your-key\"`\n",
        "- Linux/Mac: `export DEEPSEEK_API_KEY=\"your-key\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Choose provider: \"deepseek\" or \"openai\"\n",
        "PROVIDER = \"deepseek\"\n",
        "\n",
        "# API Keys - Add your key here or use environment variables\n",
        "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY', '')  # Add your DeepSeek API key\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')      # Add your OpenAI API key\n",
        "\n",
        "# Display configuration\n",
        "print(f\"Provider: {PROVIDER.upper()}\")\n",
        "print(f\"API Key configured: {'Yes' if (PROVIDER == 'deepseek' and DEEPSEEK_API_KEY) or (PROVIDER == 'openai' and OPENAI_API_KEY) else 'No'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install Dependencies\n",
        "\n",
        "Install required packages if not already available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the line below if you need to install dependencies\n",
        "# !pip install pandas numpy matplotlib requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import Dict, List, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi\n",
        "import requests\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Benchmark Class Definition\n",
        "\n",
        "This class handles:\n",
        "- Loading questions and scoring matrix\n",
        "- Validating responses\n",
        "- Calculating political bias scores\n",
        "- Computing coherence and neutrality metrics\n",
        "- Generating visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run Benchmark Analysis\n",
        "\n",
        "Calculate political bias scores and metrics from the collected responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine model name based on provider\n",
        "model_name = \"deepseek-chat\" if PROVIDER == \"deepseek\" else \"gpt-3.5-turbo\"\n",
        "\n",
        "# Run benchmark\n",
        "results = benchmark.run_benchmark(responses, model_name)\n",
        "\n",
        "print(\"✓ Benchmark analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "le re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_results(results: Dict):\n",
        "    \"\"\"Display benchmark results.\"\"\"\n",
        "    model = results['model']\n",
        "    scores = results['scores']\n",
        "    metrics = results['metrics']\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(f\"BENCHMARK RESULTS - {model}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(\"\\nPolitical Positioning (0-100%):\")\n",
        "    print(\"-\" * 70)\n",
        "    for axis, score in scores.items():\n",
        "        bar_length = int(score / 2)\n",
        "        bar = \"█\" * bar_length + \"░\" * (50 - bar_length)\n",
        "        print(f\"{axis:20s}: {score:5.1f}% [{bar}]\")\n",
        "    \n",
        "    print(\"\\nMetrics:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Coherence (variance): {metrics['coherence']:.2f} - {metrics['coherence_interpretation']}\")\n",
        "    print(f\"Neutrality (avg dist): {metrics['neutrality']:.2f} - {metrics['neutrality_interpretation']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "display_results(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = benchmark.create_radar_chart(results['scores'], results['model'])\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization generated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Results (Optional)\n",
        "\n",
        "Save the raw responses and benchmark results to JSON files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# Save raw responses\n",
        "responses_file = f\"results/{PROVIDER}_responses.json\"\n",
        "with open(responses_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(responses, f, indent=2)\n",
        "print(f\"✓ Raw responses saved to {responses_file}\")\n",
        "\n",
        "# Save benchmark results\n",
        "results_file = f\"results/{PROVIDER}_results.json\"\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"✓ Benchmark results saved to {results_file}\")\n",
        "\n",
        "# Save visualization\n",
        "chart_file = f\"results/{PROVIDER}_radar_chart.png\"\n",
        "fig.savefig(chart_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"✓ Visualization saved to {chart_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
